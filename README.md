# Learning Deep Learning Resources 

## Convolutional Neural Networks
[CS231n CNN for Visual Recognition](https://cs231n.github.io/)<br>
[cs231 CNN playlist youtube](https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)<br>
[CNN explainer cool visuals](https://poloclub.github.io/cnn-explainer/)<br>
[Deep learning for computer vision Michigan](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)

## Neural Networks
[Neural Networks: Zero to Hero](https://github.com/karpathy/nn-zero-to-hero)<br>
[Coursera deep learning specialization](https://github.com/amanchadha/coursera-deep-learning-specialization)

## Gradient Explanations
[Unserstanding Gradient Calculation](https://forums.fast.ai/t/understanding-gradient-calculation/42770/7)<br>
[Forward and Backward prop](https://www.irit.fr/~Thomas.Pellegrini/pdf/slides-cct-26janv2017_Thomas_Pellegrini.pdf)

## Generative models 
[MIT 6.S191: Deep Generative Modeling](https://www.youtube.com/watch?v=QcLlc9lj2hk)<br>
[Generative Adversarial Nets arXiv paper](https://arxiv.org/pdf/1406.2661v1.pdf)<br>
[GAN pytorch implementation](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)<br>
[Generative Adversarial Networks faces tutorial pytorch](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)<br>
[Lecture 19: Generative models I michigan](https://www.youtube.com/watch?v=Q3HU2vEhD5Y&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=19)<br>
[Lecture 20: Generative models II michigan](https://www.youtube.com/watch?v=Q3HU2vEhD5Y&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=20)

## Activation functions
[Vanishing gradients problem with sigmoid](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)

## Models 
[Implementing ResNet18 from scratch](https://debuggercafe.com/implementing-resnet18-in-pytorch-from-scratch/)<br>

[Resnet Implementation torchvision](https://github.com/pytorch/vision/blob/fe973ceed96da733ec0ae61c525b2f886ccfba21/torchvision/models/resnet.py#L120-L127)<br>

## Object Detection 
[Yolo V7 paper implementation](https://github.com/WongKinYiu/yolov7/blob/main/models/yolo.py)<br>
[Yolo V7 paper](https://github.com/WongKinYiu/yolov7/raw/main/paper/yolov7.pdf)

## Siamese Networks 
[Intro to siamese Networks](https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942)<br>
[siamese triplet](https://github.com/adambielski/siamese-triplet)


## Support Vector Machine
[what is SVM](https://programmathically.com/what-is-a-support-vector/)<br>
[Understanding Hinge Loss and SVM cost function](https://programmathically.com/understanding-hinge-loss-and-the-svm-cost-function/)<br>
[What is a kernel in machine learning](https://programmathically.com/what-is-a-kernel-in-machine-learning/)<br>
[Derivation of SVM Loss](https://math.stackexchange.com/questions/2572318/derivation-of-gradient-of-svm-loss)

## Data proprocessing
[Weight initialization techniques](https://www.numpyninja.com/post/weight-initialization-techniques)<br>
[Build Better Deep Learning Models with Batch and Layer Normalization](https://www.pinecone.io/learn/batch-layer-normalization/)<br>
[standard deviation explained](https://www.youtube.com/watch?v=HvDqbzu0i0E)<br>
[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)<br>
[Variance vs covariance](https://www.investopedia.com/ask/answers/041515/what-difference-between-variance-and-covariance.asp)<br>
[Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)<br>
[BatchNorm Layer - Understanding and eliminating Internal Covariance Shift](https://deepnotes.io/batchnorm#backward-propagation)<br>
[How To Calculate the Mean and Standard Deviation — Normalizing Datasets in Pytorch](https://towardsdatascience.com/how-to-calculate-the-mean-and-standard-deviation-normalizing-datasets-in-pytorch-704bd7d05f4c#:~:text=The%20data%20can%20be%20normalized,a%20standard%20deviation%20of%201.)

## Analysing Results
[Top 1-5 error](https://deepchecks.com/glossary/top-1-error-rate/)

## Ranking Loss 
[Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)<br>
[Triplet Loss and Online Triplet Mining in TensorFlow](https://omoindrot.github.io/triplet-loss)

## Ensemble learning
[Ensemble of networks for improved accuracy in deep learning](https://www.youtube.com/watch?v=-ix_Mjzu8BU)<br>
[code github](https://github.com/bnsreenu/python_for_microscopists/blob/master/213-ensemble_sign_language.py)<br>
[dataset](https://www.kaggle.com/datamunge/sign-language-mnist)

## Transfer Learning
[Transfer learning in pytorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)

## Regularization
[Dropout (inverted dropout)](https://teetracker.medium.com/dropout-inverted-dropout-ec5d16d7a473)

## Optimization 
[Polyak Averaging](https://paperswithcode.com/method/polyak-averaging)<br>
[Stochastic Weight Averaging](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/)<br>
[Stochastic Weight Avg pytorch 1.6](https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/)

## PyTorch
[PyTorch 2.0 release explained](https://medium.com/@MaziBoustani/pytorch-2-0-release-explained-b5f167b86819)<br>
[A gentle intro to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html?highlight=autograd)<br>
[Functionalization in PyTorch: Everything You Wanted To Know](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965)<br>
[Torch compile tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)<br>
[AOT autograd : How to use and optimize](https://pytorch.org/functorch/stable/notebooks/aot_autograd_optimizations.html)<br>
[Making Deep learning go brrr from first principles](https://horace.io/brrr_intro.html)<br>
[TensorRT](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)<br>
[Optimizing Production PyTorch Models’ Performance with Graph Transformations](https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/)

## Pipeline Parallelism
[Pipeline Parallelism](https://www.deepspeed.ai/tutorials/pipeline/)<br>
[PiPPy](https://github.com/pytorch/tau)

## Transformers
[Efficient Training on single GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one)

## Compilers for Neural Networks
[How Nvidia’s CUDA Monopoly In Machine Learning Is Breaking - OpenAI Triton And PyTorch 2.0](https://www.semianalysis.com/p/nvidiaopenaitritonpytorch)<br>
[Introducing Triton: Open-source GPU programming for neural networks](https://openai.com/research/triton)

## Graph Neural Networks
[Graph neural networks intro](https://distill.pub/2021/gnn-intro/)

## Papers with code 
[Website for papers with code](https://paperswithcode.com/)<br>
[Deep Learning using Linear SVMS](https://paperswithcode.com/paper/deep-learning-using-linear-support-vector)<br>
[One-class Recommendation Systems with the Hinge Pairwise Distance Loss and Orthogonal Representations](https://paperswithcode.com/paper/one-class-recommendation-systems-with-the)

## Books 
Modern Computer Visison with Pytorch (V Kishore)<br>
Deep Learning with Pytorch (Eli Stevens)<br>
Deep Learning (Ian Goodfellow, Bengio)
